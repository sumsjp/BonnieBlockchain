 $100 trillion of the world's industries is represented in this room today. I would like to introduce you to a very, very big GPU. This right here is, I don't know, $10 billion. The robot has a gym to go learn how to be a robot. About the same size. I think we have some special guests. Do we? Love you. Hi guys. In recent two years, we have been quickly advanced by AI. The GTC that Nvidia answered was called the world's AI wind ball. How is future AI more extravagant than you can imagine? This video released by Huang Renxun at GTC has been played dozens of millions of times. The video is very long. We try to take it out of the essence. Because I think you will definitely gain a lot. Here are the essence truths for you. Please welcome to the stage, NVIDIA founder and CEO, Jensen Wong. Welcome to GTC. These are the presenters of the non-IT industries using accelerated computing to solve problems that normal computers can't. Represented in life sciences, healthcare, genomics, transportation of course, retail, logistics, manufacturing, industrial. The gamut of industries represented is truly amazing. One hundred trillion dollars of the world's industries is represented in this room today. This is absolutely amazing. There is absolutely something happening. There is something going on. The industry is being transformed, not just ours. Because the computer industry, the computer is the single most important instrument of society today. Fundamental transformations in computing affects every industry. But how did we start? How did we get here? I made a little cartoon for you. Literally I drew this. In one page, this is NVIDIA's journey. Started in 1993. This might be the rest of the talk. 1993, this is our journey. We were founded in 1993. There are several important events that happened along the way. I'll just highlight a few. In 2006, CUDA, which has turned out to have been a revolutionary computing model. We thought it was revolutionary then. It was going to be an overnight success. And almost 20 years later it happened. We saw it coming two decades later. In 2012, AlexNet, AI and CUDA made first contact. In 2016, recognizing the importance of this computing model, we invented a brand new type of computer. We called it DGX1. One hundred and seventy teraflops in this supercomputer. Eight GPUs connected together for the very first time. I hand delivered the very first DGX1 to a startup located in San Francisco called OpenAI. DGX1 was the world's first AI supercomputer. Remember, one hundred and seventy teraflops. 2017, the transformer arrived. 2022, chat GPT captured the world's imaginations, have people realize the importance and the capabilities of artificial intelligence. And 2023, generative AI emerged. And a new industry begins. Why? Why is a new industry? Because the software never existed before. We are now producing software, using computers to write software, producing software that never existed before. It is a brand new category. It took share from nothing. It's a brand new category. And the way you produce the software is unlike anything we've ever done before. In data centers, generating tokens, producing floating point numbers at very large scale. As if in the beginning of this last industrial revolution, when people realized that you would set up factories, apply energy to it. And this invisible valuable thing called electricity came out. AC generators. And a hundred years later, two hundred years later, we are now creating new types of electrons, tokens using infrastructure we call factories, AI factories, to generate this new incredibly valuable thing called artificial intelligence. A new industry has emerged. We created a processor for the generative AI era. And one of the most important parts of it is content token generation. We call it, this format is FP4. The rate at which we're advancing computing is insane. And it's still not fast enough, so we built another chip. This chip is just an incredible chip. We call it the MVLink switch. It's 50 billion transistors. It's almost the size of Hopper all by itself. This switch chip has four MVLinks in it, each 1.8 terabytes per second. And it has computation in it, as I mentioned. What is this chip for? If we were to build such a chip, we can have every single GPU talk to every other GPU at full speed at the same time. You can build a system that looks like this. Now this system, this system is kind of insane. This is one DGX. This is what a DGX looks like now. Just so you know, there are only a couple, two, three ExaFlops machines on the planet as we speak. And so this is an ExaFlops AI system in one single rack. Accelerated computing has reached the tipping point. General purpose computing has run out of steam. We need another way of doing computing so that we can continue to scale, so that we can continue to drive down the cost of computing, so that we can continue to consume more and computing while being sustainable. Accelerated computing is a dramatic speed up over general purpose computing. And in every single industry we engage, and I'll show you many, the impact is dramatic. But in no industry is it more important than our own. The industry of using simulation tools to create products. In this industry, it is not about driving down the cost of computing, it's about driving up the scale of computing. We would like to be able to simulate the entire product that we do, completely, in full fidelity, completely digitally, and essentially what we call digital twins. We would like to design it, build it, simulate it, operate it, completely digitally. In order to do that, we need to accelerate an entire industry. One of the industries that benefited tremendously from scale, and you all know this one very well, large language models. The day after the transformer was invented, we were able to scale large language models at incredible rates, effectively doubling every six months. Now how is it possible that by doubling every six months, that we have grown the industry, we have grown the computational requirements so far? And the reason for that is quite simply this. If you double the size of the model, you double the size of your brain, you need twice as much information to go fill it. And so every time you double your parameter count, you also have to appropriately increase your training token count. The combination of those two numbers becomes the computation scale you have to support. The latest, the state of the art open AI model is approximately 1.8 trillion parameters. 1.8 trillion parameters required several trillion tokens to go train. So a few trillion parameters on the order of, a few trillion tokens on the order of, when you multiply the two of them together, approximately 30, 40, 50 billion quadrillion floating point operations per second. Now we just have to do some CO math right now, just hang with me. So you have 30 billion quadrillion. A quadrillion is like a peta. And so if you had a peta flop GPU, you would need 30 billion seconds to go compute, to go train that model. 30 billion seconds is approximately 1,000 years. Well 1,000 years, it's worth it. I'd like to do it sooner, but it's worth it. Which is usually my answer when most people tell me, hey, how long is it going to take to do something? So we've got 20 years, it's worth it. But can we do it next week? And so 1,000 years, 1,000 years, so what we need, what we need are bigger GPUs. We need much, much bigger GPUs. We recognized this early on, and we realized that the answer is to put a whole bunch of GPUs together. And of course, innovate a whole bunch of things along the way, like inventing tensor cores, advancing NVLINK so that we could create essentially virtually giant GPUs, and connecting them all together with amazing networks from a company called Melanox, Infiniband, so that we could create these giant systems. And so DGX1 was our first version, but it wasn't the last. We built supercomputers all the way, all along the way. In 2021, we had Selene, 4,500 GPUs or so. And then in 2023, we built one of the largest AI supercomputers in the world. It's just come online, EOS. And as we're building these things, we're trying to help the world build these things. And in order to help the world build these things, we got to build them first. We build the chips, the systems, the networking, all of the software necessary to do this. You should see these systems. Imagine writing a piece of software that runs across the entire system, distributing the thousands of GPUs, but inside are thousands of smaller GPUs, millions of GPUs to distribute work across all of that and to balance the workload so that you can get the most energy efficiency, the best computation time, keep your costs down. And so those fundamental innovations is what got us here. And here we are, as we see the miracle of chat GPT emerge in front of us, we also realize we have a long ways to go. We need even larger models. We're going to train it with multi-modality data, not just text on the internet, but we're going to train it on text and images and graphs and charts. And just as we learned, watching TV. And so there's going to be a whole bunch of watching video so that these models can be grounded in physics, understands that an arm doesn't go through a wall. And so these models would have common sense by watching a lot of the world's video combined with a lot of the world's languages. We'll use things like synthetic data generation, just as you and I do. When we try to learn, we might use our imagination to simulate how it's going to end up, just as I did when I was preparing for this keynote. I was simulating it all along the way. I hope it's going to turn out as well as I had it in my head. And so where were we? We're sitting here using synthetic data generation. We're going to use reinforcement learning. We're going to practice it in our mind. We're going to have AI working with AI training each other, just like student, teacher, debater, all of that is going to increase the size of our model. It's going to increase the amount of data that we have, and we're going to have to build even bigger GPUs. Hopper is fantastic, but we need bigger GPUs. And so ladies and gentlemen, I would like to introduce you to a very, very big GPU. Named after David Blackwell, mathematician, game theorist, probability. We thought it was a perfect name. Blackwell, ladies and gentlemen, enjoy this. Thank you. Blackwell is not a chip. Blackwell is the name of a platform. People think we make GPUs, and we do, but GPUs don't look the way they used to. This is Hopper. Hopper changed the world. This is Blackwell. It's okay, Hopper. You're very good. Good girl. 208 billion transistors, and so you can see, I can see that there's a small line between two dyes. This is the first time two dyes have a button like this together in such a way that the two dyes think it's one chip. There's 10 terabytes of data between them, 10 terabytes per second. So that these two sides of the Blackwell chip have no clue which side they're on. There's no memory locality issues, no cache issues. It's just one giant chip, and it goes into two types of systems. The first one is for the function compatible to Hopper. And so you slide on Hopper, and you push in Blackwell. That's the reason why one of the challenges of ramping is going to be so efficient. There are installations of Hoppers all over the world, and they could be the same infrastructure, same design, the power, the electricity, the thermals, the software, identical, push it right back. This is a Hopper version for the current HGX configuration. And this is what the other, the second Hopper looks like this. Now this is a prototype board. This is a fully functioning board. And I'll just be careful here. This right here is, I don't know, $10 billion? The second one's five. It gets cheaper after that, so any customers in the audience, it's okay. The Gray CPU has a super fast chip to chip link. What's amazing is this computer is the first of its kind where this much computation, first of all, fits into this small of a place. Second, it's memory coherent. They feel like they're just one big happy family working on one application together. So it turns out, it turns out all of the specs is fantastic, but we need a whole lot of new features. In order to push the limits beyond, if you will, the limits of physics, we would like to always get a lot more X-factors. And so one of the things that we did was we invented another transformer engine, another transformer engine, the second generation. It has the ability to dynamically and automatically rescale and recast numerical formats to a lower precision whenever it can. Remember, artificial intelligence is about probability. And so you kind of have approximately 1.7 times approximately 1.4 to be approximately something else. Does that make sense? So the ability for the mathematics to retain the precision and the range necessary in that particular stage of the pipeline, super important. And so this is not just about the fact that we designed a smaller ALU. The world is not quite that simple. You've got to figure out when you can use that across a computation that is thousands of GPUs. It's running for weeks and weeks and weeks. And you want to make sure that the training job is going to converge. And so this new transformer engine, we have a fifth generation NVLink. It's now twice as fast as Hopper, but very importantly, it has computation in the network. And the reason for that is because when you have so many different GPUs working together, we have to share our information with each other. We have to synchronize and update each other. And every so often, we have to reduce the partial products and then rebroadcast out the partial products, the sum of the partial products back to everybody else. And so there's a lot of what is called all reduce and all to all and all gather. It's all part of this area of synchronization and collectives so that we can have GPUs working with each other. Having extraordinarily fast links and being able to do mathematics right in the network allows us to essentially amplify even further. So even though it's 1.8 terabytes per second, it's effectively higher than that. And so it's many times that of Hopper. The likelihood of a supercomputer running for weeks on end is approximately zero. And the reason for that is because there's so many components working at the same time. The statistic, the probability of them working continuously is very low. And so we need to make sure that whenever there is a checkpoint and restart as often as we can. But if we have the ability to detect a weak chip or a weak node early, we can retire it and maybe swap in another processor. That ability to keep the utilization of the supercomputer high, especially when you just spent $2 billion building it, is super important. And so we put in a RAS engine, a reliability engine that does 100% self-test, in-system of every single gate, every single bit of memory on the Blackwell chip and all the memory that's connected to it. It's almost as if we shipped with every single chip its own advanced tester that we test our chips with. This is the first time we're doing this. Super excited about it. Secure AI. Obviously, you've just spent hundreds of millions of dollars creating a very important AI. The code, the intelligence of that AI is encoded into parameters. You want to make sure that on the one hand, you don't lose it. On the other hand, it doesn't get contaminated. And so we now have the ability to encrypt data, of course, at rest, but also in transit and while it's being computed. It's all encrypted. And so we now have the ability to encrypt in transmission. And when we're computing it, it is in a trusted, trusted environment, trusted engine environment. The last thing is decompression. Competing data in and out of these nodes when the compute is so fast becomes really essential. And so we've put in a high line speed compression engine and effectively moves data 20 times faster in and out of these computers. These computers are so powerful and there's such a large investment, the last thing we want to do is have them be idle. And so all of these capabilities are intended to keep Blackwell fed and as busy as possible. Whenever you use a computer with AI on the other side, when you're chatting with the chatbot, when you're asking it to review or make an image, remember in the back is a GPU generating tokens. Some people call it inference, but it's more appropriately generation. The way that computing has done in the past was retrieval. You would grab your phone, you would touch something, some signals go off, basically an email goes off to some storage somewhere. There's pre-recorded content, somebody wrote a story or somebody made an image or somebody recorded a video. That record pre-recorded content is then streamed back to the phone and recomposed in a way based on a recommender system to present the information to you. You know that in the future, the vast majority of that content will not be retrieved. And the reason for that is because that was pre-recorded by somebody who doesn't understand the context, which is the reason why we have to retrieve so much content. If you can be working with an AI that understands the context, who you are, for what reason you're fetching this information and produces the information for you just the way you like it. The amount of energy we save, the amount of networking bandwidth we save, the amount of waste of time we save will be tremendous. The future is generative, which is the reason why we call it generative AI, which is the reason why this is a brand new industry. The way we compute is fundamentally different. Somebody used to say, you know, you guys make GPUs and we do, but this is what a GPU looks like to me. When somebody says GPU, I see this. Two years ago when I saw a GPU, it was the HGX. It was 70 pounds, 35,000 parts. Our GPUs now are 600,000 parts and 3,000 pounds. 3,000 pounds. 3,000 pounds. That's kind of like the weight of a carbon fiber Ferrari. I don't know if that's a useful metric, but... So this is what a DGX looks like. Now let's see what it looks like in operation. Okay. How do we put this to work and what does that mean? Well, if you were to train a GPT model, 1.8 trillion parameter model, it took about, apparently about three to five months or so with 25,000 amperes. If we were to do it with Hopper, it would probably take something like 8,000 GPUs and it would consume 15 megawatts. 8,000 GPUs and 15 megawatts. It would take 90 days, about three months. And that would allow you to train something that is this groundbreaking AI model. And this is obviously not as expensive as anybody would think, but it's 8,000 GPUs. It's still a lot of money. And so 8,000 GPUs, 15 megawatts. If you were to use Blackwell to do this, it would only take 2,000 GPUs. 2,000 GPUs, same 90 days, but this is the amazing part. Only four megawatts of power. So from 15... That's right. And that's our goal. Our goal is to continuously drive down the cost and the energy that are directly proportional to each other, cost and energy associated with the computing so that we can continue to expand and scale up the computation that we have to do to train the next generation models. Well, this is training. Inference or generation is vitally important going forward. Probably some half of the time that Nvidia GPUs are in the cloud these days, it's being used for token generation. They're either doing copilot this or chat GPT that or all these different models that are being used when you're interacting with it or generating images or generating videos, generating proteins, generating chemicals. There's a bunch of generation going on. All of that is in the category of computing we call inference. But inference is extremely hard for large language models because these large language models have several properties. One, they're very large. And so it doesn't fit on one GPU. This is... Imagine Excel doesn't fit on one GPU. And imagine some application you're running on a daily basis doesn't fit on one computer. Like a video game doesn't fit on one computer. And most, in fact, do. And many times in the past, in hyperscale computing, many applications for many people fit on the same computer. And now all of a sudden, this one inference application where you're interacting with this chatbot, that chatbot requires a supercomputer in the back to run it. And that's the future. The future is generative with these chatbots. And these chatbots are trillions of tokens, trillions of parameters, and they have to generate tokens at interactive rates. Now what does that mean? Well, three tokens is about a word. You know, the space, the final frontier, these are the adventures. That's like 80 tokens. Okay? I don't know if that's useful to you. And so the art of communications is selecting good analogies. Yeah, this is not going well. I don't know what he's talking about. Never seen Star Trek. And so here we are, we're trying to generate these tokens. When you're interacting with it, you're hoping that the tokens come back to you as quickly as possible, and this is as quickly as you can read it. And so the ability for generation tokens is really important. You have to paralyze the work of this model across many, many GPUs so that you could achieve several things. On the one hand, you would like throughput because that throughput reduces the overall cost per token of generating. So your throughput dictates the cost of delivering the service. On the other hand, you have another interactive rate, which is another tokens per second, where it's about per user, and that has everything to do with quality of service. And so these two things compete against each other. And we have to find a way to distribute work across all of these different GPUs and paralyze it in a way that allows us to achieve both. And it turns out the search space is enormous. In the future, data centers are going to be thought of, as I mentioned earlier, as an AI factory. An AI factory's goal in life is to generate revenues, generate, in this case, intelligence in this facility, not generating electricity as in AC generators, but the last industrial revolution and this industrial revolution, the generation of intelligence. And so this ability is super, super important. The excitement of Blackwell is really off the charts. You know, when we first, when we first, you know, this is a year and a half ago, two years ago, I guess two years ago, when we first started to go to market with Hopper, you know, we had the benefit of two CSPs joined us in lunch and we were delighted. And so we had two customers. We have more now. Unbelievable excitement for Blackwell. Everything in our company has a digital twin. And in fact, this digital twin idea is really spreading and it helps companies build very complicated things perfectly the first time. And what could be more exciting than creating a digital twin to build a computer that was built in a digital twin? And so let me show you what Wistron is doing. To meet the demand for Nvidia accelerated computing, Wistron, one of our leading manufacturing partners, is building digital twins of Nvidia DGX and HGX factories using custom software developed with Omniverse, SDKs and APIs. For their newest factory, Wistron started with a digital twin to virtually integrate their multi CAD and process simulation data into a unified view. Testing and optimizing layouts in this physically accurate digital environment increased worker efficiency by 51%. During construction, the Omniverse digital twin was used to verify that the physical build matched the digital plans. Identifying any discrepancies early has helped avoid costly change orders. And the results have been impressive. Using a digital twin helped bring Wistron's factory online in half the time, just two and a half months instead of five. In operation, the Omniverse digital twin helps Wistron rapidly test new layouts to accommodate new processes or improve operations in the existing space and monitor real-time operations using live IoT data from every machine on the production line, which ultimately enabled Wistron to reduce end-to-end cycle times by 50% and defect rates by 40%. With Nvidia AI and Omniverse, Nvidia's global ecosystem of partners are building a new era of accelerated AI-enabled digitalization. That's how we are. That's the way it's going to be in the future. We're going to manufacture everything digitally first, and then we'll manufacture it physically. 2012, AlexNet. You put a cat into this computer, and it comes out and it says, cat. And we said, oh my god, this is going to change everything. You take one million numbers, you take one million numbers across three channels, RGB. These numbers make no sense to anybody. You put it into this software, and it compresses, it dimensionally reduces it. It reduces it from a million dimensions, a million dimensions. It turns it into three letters, one vector, one number. And it's generalized. You could have the cat be different cats, and you could have it be the front of the cat and the back of the cat. And you look at this thing, you say, unbelievable. You mean any cats? Yeah, any cat. And it was able to recognize all these cats. And we realized how it did it. Systematically, structurally, it's scalable. How big can you make it? Well, how big do you want to make it? And so we imagined that this is a completely new way of writing software. And now today, as you know, you can have, you type in the word, CAT. And what comes out is a cat. It went the other way. Am I right? Unbelievable. How is it possible? That's right. How is it possible you took three letters and you generated a million pixels from it and it made sense? Well, that's the miracle. And here we are, just literally 10 years later, 10 years later, where we recognize text, we recognize images, we recognize videos and sounds and images. Not only do we recognize them, we understand their meaning. We understand the meaning of the text. That's the reason why it can chat with you. It can summarize for you. It understands the text. It understood not just recognizes the English, it understood the English. It doesn't just recognize the pixels, it understood the pixels. And you can even condition it between two modalities. You can have language condition image and generate all kinds of interesting things. Well, if you can understand these things, what else can you understand that you've digitized? The reason why we started with text and images is because we digitized those. But what else have we digitized? Well, it turns out we digitized a lot of things, proteins and genes and brain waves. Anything you can digitize, so long as they're structured, we can probably learn some patterns from it. And if we can learn the patterns from it, we can understand its meaning. If we can understand its meaning, we might be able to generate it as well. And so therefore, the generative AI revolution is here. Well, what else can we generate? What else can we learn? Well, one of the things that we would love to learn, we would love to learn, is we would love to learn climate. We would love to learn extreme weather. We would love to learn how we can predict future weather at regional scales at sufficiently high resolution such that we can keep people out of harm's way before harm comes. Extreme weather cost the world $150 billion, surely more than that, it's not evenly distributed. $150 billion is concentrated in some parts of the world and, of course, to some people of the world. We need to adapt and we need to know what's coming. And so we're creating Earth-2, a digital twin of the Earth for predicting weather. And we've made an extraordinary invention called CoreDiff, the ability to use generative AI to predict weather at extremely high resolution. Let's take a look. As the Earth's climate changes, AI-powered weather forecasting is allowing us to more accurately predict and track severe storms like Super Typhoon Chanthu, which caused widespread damage in Taiwan and the surrounding region in 2021. Current AI forecast models can accurately predict the track of storms, but they are limited to 25-kilometer resolution, which can miss important details. NVIDIA's CoreDiff is a revolutionary new generative AI model trained on high-resolution radar-assimilated, wharf weather forecasts and air-refine reanalysis data. NVIDIA Healthcare. Virtual screening for new medicines is a computationally intractable problem. Existing techniques can only scan billions of compounds and require days on thousands of standard compute nodes to identify new drug candidates. NVIDIA BioNemo NIMs enable a new generative screening paradigm. Using NIMs for protein structure prediction with alpha-fold, molecule generation with mole-MIM, and docking with diff-dock, we can now generate and screen candidate molecules in a matter of minutes. Mole-MIM can connect to custom applications to steer the generative process, iteratively optimizing for desired properties. These applications can be defined with BioNemo microservices or built from scratch. Here, a physics-based simulation optimizes for a molecule's ability to bind to a target protein while optimizing for other favorable molecular properties in parallel. Mole-MIM generates high-quality drug-like molecules that bind to the target and are synthesizable, translating to a higher probability of developing successful medicines faster. BioNemo is enabling a new paradigm in drug discovery with NIMs, providing on-demand microservices that can be combined to build powerful drug discovery workflows like de novo protein design or guided molecule generation for virtual screening. BioNemo NIMs are helping researchers and developers reinvent computational drug design. Computer vision models, robotics models, and even, of course, some really, really terrific open-source language models. These models are groundbreaking. However, it's hard for companies to use. How would you use it? How would you bring it into your company and integrate it into your workflow? How would you package it up and run it? Remember, earlier I just said that inference is an extraordinary computation problem. How would you do the optimization for each and every one of these models and put together the computing stack necessary to run that supercomputer so that you can run these models in your company? And so we have a great idea. We're going to invent a new way for you to receive and operate software. This software comes basically in a digital box. We call it a container. And we call it the NVIDIA inference microservice, a NIM. And let me explain to you what it is. A NIM. It's a pre-trained model. So it's pretty clever. And it is packaged and optimized to run across NVIDIA's install base, which is very, very large. What's inside it is incredible. You have all these pre-trained state-of-the-art open-source models. They could be open-source. They could be from one of our partners. It could be created by us, like NVIDIA Moment. It is packaged up with all of its dependencies. So CUDA, the right version, CUDNN, the right version, TensorRT, LLM, distributing across the multiple GPUs, Triton inference server, all completely packaged together. It's optimized depending on whether you have a single GPU, multi-GPU, or multi-node of GPUs. It's optimized for that. And it's connected up with APIs that are simple to use. These packages, incredible bodies of software, will be optimized and packaged and we'll put it on a website. You can download it. You can take it with you. You can run it in any cloud. You can run it in your own data center. You can run it in workstations if it fit. And all you have to do is come to ai.nvidia.com. We call it NVIDIA Inference Microservice, but inside the company, we all call it NIMS. We have a service called NEMO Microservice that helps you curate the data, prepare the data so that you can teach this onboard this AI. You fine-tune them, and then you guardrail it. You can even evaluate the answer, evaluate its performance against other examples. And so we are effectively an AI foundry. We will do for you and the industry on AI what TSMC does for us, building chips. And so we go to TSMC with our big ideas. They manufacture it, and we take it with us. And so exactly the same thing here. AI foundry, and the three pillars are the NIMS, NEMO Microservice, and DGX Cloud. Remember, inside our company, the vast majority of our data is not in the cloud. It's inside our company. It's been sitting there, you know, being used all the time, and gosh, it's basically NVIDIA's intelligence. We would like to take that data, learn its meaning, like we learned the meaning of almost anything else that we just talked about. Learn its meaning, and then re-index that knowledge into a new type of database called a vector database. And so you essentially take structured data or unstructured data, you learn its meaning, you encode its meaning, so now this becomes an AI database, and that AI database, in the future, once you create it, you can talk to it. And so let me give you an example of what you could do. So suppose you create, you've got a whole bunch of multi-modality data, and one good example of that is PDF. So you take the PDF, you take all of your PDFs, all your favorite, you know, the stuff that is proprietary to you, critical to your company, you can encode it, just as we encode the pixels of a cat, and it becomes the word cat, we can encode all of your PDF, and it turns into vectors that are now stored inside your vector database. It becomes the proprietary information of your company. And once you have that proprietary information, you can check to it. It's a smart database. So you just chat with data. And how much more enjoyable is that? You know, for our software team, you know, they just chat with the bugs database, you know, how many bugs were there last night? Are we making any progress? And then after you're done talking to this bugs database, you need therapy. And so we have another chat bot for you. You can do it. The enterprise IT industry is sitting on the gold mine. It's a gold mine because they have so much understanding of the way work is done. They have all these amazing tools that have been created over the years, and they're sitting on a lot of data. If they could take that gold mine and turn them into copilots, these copilots could help us do things. And so just about every IT franchise, IT platform in the world that has valuable tools that people use is sitting on a gold mine for copilots, and they would like to build their own copilots and their own chat bots. We're announcing that Nvidia AI Foundry is working with some of the world's great companies. SAP generates 87% of the world's global commerce. Basically the world runs on SAP. We run on SAP. Nvidia and SAP are building SAP Jewel copilots using Nvidia Nemo and DGX Cloud. ServiceNow, they run 85% of the world's Fortune 500 companies run their people and customer service operations on ServiceNow. And they're using Nvidia AI Foundry to build ServiceNow assist virtual assistants. Cohesity backs up the world's data. They're sitting on a gold mine of data. Hundreds of exabytes of data, over 10,000 companies. Nvidia AI Foundry is working with them, helping them build their Gaia generative AI agent. Snowflake is a company that stores the world's digital warehouse in the cloud and serves over 3 billion queries a day for 10,000 enterprise customers. Snowflake is working with Nvidia AI Foundry to build copilots with Nvidia Nemo and NIMS. NetApp, nearly half of the files in the world are stored on-prem on NetApp. Nvidia AI Foundry is helping them build chatbots and copilots like those vector databases and retrievers with Nvidia Nemo and NIMS. And we have a great partnership with Dell. Everybody who is building these chatbots and generative AI, when you're ready to run it, you're going to need an AI factory. And nobody is better at building end-to-end systems of very large scale for the enterprise than Dell. And so anybody, any company, every company will need to build AI factories. And it turns out that Michael is here. He's happy to take your order. And let's talk about the next wave of robotics, the next wave of AI, robotics, physical AI. So far, all of the AI that we've talked about is one computer. Data comes into one computer, lots of the world's, if you will, experience in digital text form. The AI imitates us by reading a lot of the language to predict the next words. It's imitating you by studying all of the patterns and all the other previous examples. Of course, it has to understand context and so on and so forth. But once it understands the context, it's essentially imitating you. We take all of the data, we put it into a system like DGX, we compress it into a large language model. Trillions and trillions of parameters become billions and billions. Trillions of tokens becomes billions of parameters. These billions of parameters becomes your AI. Well, in order for us to go to the next wave of AI, where the AI understands the physical world, we're going to need three computers. The first computer is still the same computer. It's that AI computer that now is going to be watching video and maybe it's doing synthetic data generation. Maybe there's a lot of human examples, just as we have human examples in text form, we're going to have human examples in articulation form. And the AIs will watch us, understand what is happening, and try to adapt it for themselves into the context. And because it can generalize with these foundation models, maybe these robots can also perform in the physical world fairly generally. So I just described in very simple terms essentially what just happened in large language models, except the chat GPT moment for robotics may be right around the corner. And so we've been building the end-to-end systems for robotics for some time. I'm super, super proud of the work. We have the AI system, DGX. We have the lower system, which is called AGX, for autonomous systems, the world's first robotics processor. When we first built this thing, people were like, what are you guys building? It's an SOC. It's one chip. It's designed to be very low power, but it's designed for high speed sensor processing and AI. And so if you want to run transformers in a car or you want to run transformers in anything, that moves, we have the perfect computer for you. It's called the Jetson. And so the DGX on top for training the AI, the Jetson is the autonomous processor. And in the middle, we need another computer. Whereas large language models have the benefit of you providing your examples and then doing reinforcement learning human feedback. What is the reinforcement learning human feedback of a robot? Well, it's reinforcement learning physical feedback. That's how you align the robot. That's how the robot knows that as it's learning these articulation capabilities and manipulation capabilities, it's going to adapt properly into the laws of physics. And so we need a simulation engine that represents the world visually for the robot so that the robot has a gym to go learn how to be a robot. We call that virtual world Omniverse. And the computer that runs Omniverse is called OVX. And OVX, the computer itself is hosted in the Azure cloud. And so basically, we built these three things, these three systems. On top of it, we have algorithms for every single one. Now, I'm going to show you one super example of how AI and Omniverse are going to work together. The example I'm going to show you is kind of insane, but it's going to be very, very close to tomorrow. Basically, the system I just described will have Omniverse cloud that's hosting the virtual simulation and AI running on DGX cloud. And all of this is running in real time. Let's take a look. The future of heavy industries starts as a digital twin. The AI agents helping robots, workers, and infrastructure navigate unpredictable events in complex industrial spaces will be built and evaluated first in sophisticated digital twins. This Omniverse digital twin of a 100,000 square foot warehouse is operating as a simulation environment that integrates digital workers, AMRs running the NVIDIA Isaac receptor stack, centralized activity maps of the entire warehouse from 100 simulated ceiling mount cameras using NVIDIA Metropolis and AMR route planning with NVIDIA Co-Opt. Software in loop testing of AI agents in this physically accurate simulated environment enables us to evaluate and refine how the system adapts to real world unpredictability. Here an incident occurs along this AMR's planned route, blocking its path as it moves to pick up a pallet. NVIDIA Metropolis updates and sends a real time occupancy map to Co-Opt where a new optimal route is calculated. The AMR is enabled to see around corners and improve its mission efficiency. With generative AI powered Metropolis vision foundation models, operators can even ask questions using natural language. The visual model understands nuanced activity and can offer immediate insights to improve operations. All of the sensor data is created in simulation and passed to the real time AI running as NVIDIA Inference Microservices or NIMS. And when the AI is ready to be deployed in the physical twin, the real warehouse, we connect Metropolis and Isaac NIMS to real sensors with the ability for continuous improvement of both the digital twin and the AI models. We have a great partnership with Siemens. Siemens is the world's largest industrial engineering and operations platform. You've seen now so many different companies in the industrial space. Heavy Industries is one of the greatest final frontiers of IT. And we finally now have the necessary technology to go and make a real impact. Siemens is building the industrial metaverse. And today we're announcing that Siemens is connecting their crown jewel accelerator to NVIDIA Omniverse. Let's take a look. Siemens technology is transformed every day for everyone. Team Center X, our leading product lifecycle management software from the Siemens accelerator platform, is used every day by our customers to develop and deliver products at scale. Now we are bringing the real and digital worlds even closer by integrating NVIDIA AI and Omniverse technologies into Team Center X. Omniverse APIs enable data interoperability and physics-based rendering to industrial scale design and manufacturing projects. Our customers, HD&A, market leader in sustainable ship manufacturing, builds ammonia and hydrogen power chips, often comprising over 7 million discrete parts. With Omniverse APIs, Team Center X lets companies like HD&A unify and visualize these massive engineering data sets interactively. It integrates generative AI to generate 3D objects or HDRI backgrounds to see their projects in context. The result, an ultra-intuitive photoreal physics-based digital twin that eliminates waste and errors, delivering huge savings in cost and time. We are building this for collaboration, whether across more Siemens accelerator tools like Siemens NX or Star CCM+, or across teams working on their favorite devices in the same scene together. And this is just the beginning. Working with NVIDIA, we will bring accelerator computing, generative AI, and Omniverse integration across the Siemens accelerator portfolio. Once you connect everything together, it's insane how much productivity you can get. And it's just really, really wonderful. All of a sudden, everybody's operating on the same ground truth. You don't have to exchange data and convert data, make mistakes. Everybody is working on the same ground truth. From the design department to the art department, the architecture department, all the way to the engineering and even the marketing department. Today, we're announcing that Omniverse Cloud streams to the Vision Pro. It is very, very strange that you walk around virtual doors when I was getting out of that car and everybody does it. It is really, really quite amazing. Vision Pro, connected to Omniverse, portals you into Omniverse. And because all of these CAD tools and all these different design tools are now integrated and connected to Omniverse, you can have this type of workflow. Really incredible. This is NVIDIA Project Group. A general purpose foundation model for humanoid robot learning. The group model takes multimodal instructions and past interactions as input and produces the next action for the robot to execute. We developed Isaac Lab, a robot learning application to train group on Omniverse Isaac Sim. And we scale out with Osmo, a new compute orchestration service that coordinates workflows across DGX systems for training and OVX systems for simulation. The group model will enable a robot to learn from a handful of human demonstrations so it can help with everyday tasks and emulate human movement just by observing us. All this incredible intelligence is powered by the new Jetson Thor Robotics chips designed for group, built for the future. With Isaac Lab, Osmo and group, we're providing the building blocks for the next generation of AI powered robotics. About the same size. The soul of NVIDIA. The intersection of computer graphics, physics, artificial intelligence. It all came to bear at this moment. The name of that project, General Robotics 003. I know. Super good. Super good. Super good. Well, I think we have some special guests. Do we? Hey guys. So I understand you guys are powered by Jetson. They're powered by Jetsons. Little Jetson robotics computers inside. They learn to walk in Isaac Sim. Ladies and gentlemen, this is Orange and this is the famous Green. They are the BDX robots of Disney. Amazing Disney research. Come on you guys, let's wrap up. Let's go. Five things. Where are you going? I said right here. Don't be afraid. Come here Green. Hurry up. What are you saying? No, it's not time to eat. It's not time to eat. I'll give you a snack in a moment. Let me finish up real quick. Come on Green, hurry up. Stop wasting time. This is what we announce to you today. This is Blackwell. This is the platform. Amazing, amazing processors. NVLink switches, networking systems. And the system design is a miracle. This is Blackwell. And this to me is what a GPU looks like in my mind.